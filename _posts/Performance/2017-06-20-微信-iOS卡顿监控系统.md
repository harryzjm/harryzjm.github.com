---
layout: post
title: 微信-iOS卡顿监控系统
category: Performance
tags: Swift Define
keywords: Jekyll,Github
description: 
---

__Posted by [WeMobileDev](https://mp.weixin.qq.com/s?__biz=MzAwNDY1ODY2OQ==&mid=207890859&idx=1&sn=e98dd604cdb854e7a5808d2072c29162&scene=21#wechat_redirect)__  


## 引子



微信 iOS 团队在值班的时候，时不时会收到这样的卡顿反馈：“用户A 刚才碰到从后台切换前台卡了一下，最近偶尔会遇到几次”、“用户B 反馈点对话框卡了五六秒”、“现网有用户反馈切换 tab 很卡”。

这些反馈有几个特点，导致跟进困难：

1. 不易重现。可能是特定用户的手机上才有问题，由于种种原因这个手机不能拿来调试；也有可能是特定的时机才会出问题，过后就不能重现了（例如线程抢锁）。

2. 操作路径长，日志无法准确打点

对于这些界面卡顿反馈，通常我们拿用户日志作用不大，增加日志点也用处不大。只能不断重试希望能够重现出来，或者埋头代码逻辑中试图能找的蛛丝马迹。随着微信的发展普及，这类问题积累得越来越多，为了攻城狮的尊严，我们感觉到有必要专门处理一下了。




## 原理



在开始之前，我们先思考一下，界面卡顿是由哪些原因导致的？

* 死锁：主线程拿到锁 A，需要获得锁 B，而同时某个子线程拿了锁 B，需要锁 A，这样相互等待就死锁了。

* 抢锁：主线程需要访问 DB，而此时某个子线程往 DB 插入大量数据。通常抢锁的体验是偶尔卡一阵子，过会就恢复了。

* 主线程大量 IO：主线程为了方便直接写入大量数据，会导致界面卡顿。

* 主线程大量计算：算法不合理，导致主线程某个函数占用大量 CPU。

* 大量的 UI 绘制：复杂的 UI、图文混排等，带来大量的 UI 绘制。




针对这些原因，我们可以怎么定位问题呢？

* 死锁一般会伴随 crash，可以通过 crash report 来分析。

* 抢锁不好办，将锁等待时间打出来用处不大，我们还需要知道是**谁占了锁**。

* 大量 IO 可以在函数开始结束打点，将占用时间打到日志中。

* 大量计算同理可以将耗时打到日志中。

* 大量 UI 绘制一般是必现，还好办；如果是偶现的话，想加日志点都没地方，因为是**慢在系统函数里面**。




如果可以将当时的线程堆栈捕捉下来，那么上述难题都迎刃而解。主线程在什么函数哪一行卡住，在等什么锁，而这个锁又是被哪个子线程的哪个函数占用，有了堆栈，我们都可以知道。自然也能知道是慢在UI绘制，还是慢在我们的代码。

所以，思路就是**起一个子线程，监控主线程的活动情况，如果发现有卡顿，就将堆栈 dump 下来**。

流程图描述如下：

![](/assets/postAssets/2017/14979381571425.webp)




## 细节



原理一旦讲出来，好像也不复杂。魔鬼都是隐藏在细节中，效果好不好，完全由实现细节决定。具体到卡顿检测，有几个问题需要仔细处理：

* 怎么知道主线程发生了卡顿？

* 子线程以什么样的策略和频率来检测主线程？这个是要发布到现网的，如果处理不好，带来明显的性能损耗（尤其是电量），就不能接受了。

* 堆栈上报了上来怎么分类？直接用 crash report 的分类不适合。

* 卡顿 dump 下来的堆栈会有多频繁？数据量会有多大？

* 全量上报还是抽样上报？怎么在问题跟进与节省流量直接平衡？



**1\. 判断标准**

怎么判断主线程是不是发生了卡顿？一般来说，用户感受得到的卡顿大概有三个特征：

* FPS 降低

* CPU 占用率很高

* 主线程 Runloop 执行了很久




看起来 FPS 能够兼容后面两个特征，但是在实际操作过程中发现 FPS 不好衡量，抖动比较大。而对于抢锁或大量 IO 的情况，光有 CPU 是不行的。所以我们实际上用到的是下面两个准则：

* CPU 占用超过了100%

* 主线程 Runloop 执行了超过2秒



**2\. 检测策略**

为了降低检测带来的性能损耗，我们仔细设计了检测线程的策略：

* 内存 dump：每**1秒检查一次**，如果检查到主线程卡顿，就将所有线程的函数调用堆栈 dump 到内存中。

* 文件 dump：如果内存 dump 的堆栈跟上次捕捉到的不一样，则 dump 到文件中；否则按照斐波那契数列将**检查时间递增**（1，1，2，3，5，8…）直到没有遇到卡顿或卡顿堆栈不一样。这样能够避免同一个卡顿写入多个文件的情况，也能避免检测线程围着同一个卡顿空转的情况。

**3\. 分类方法**

直接用 crash report 的分类方法是不行的，这个很好理解：最终卡在 lock 函数的卡顿，外面可能是很多不同的业务，例如可能是读取消息，可能是读取联系人，等等。卡顿监控需要仔细定义自己的分类规则。可以是从调用堆栈的最外层开始归类，或者是取中间一部分归类，或者是取最里面一部分归类。各有优缺点：

* 最外层归类：能够将同一入口的卡顿归类起来。缺点是层数不好定，可能外面十来层都是系统调用，也有可能第一层就是微信的函数了。

* 中间层归类：能够根据事先划分好的“特征值”来归类。缺点是“特征值”不好定，如果要做到自动学习生成的话，对后台分析系统要求太高了。

* 最内层归类：能够将同一原因的卡顿归类起来。缺点是同一分类可能包含不同的业务。




综合考虑并一一尝试之后，我们采用了最内层归类的优化版，亦即进行二级归类。

第一级按照最内倒数2层归类，这样能够将同一原因的卡顿集中起来；

第二级分类是从第一级点击进来，然后从最内层倒数4层进行归类，这样能够将同一原因的不同业务分散归类起来。




最终效果如下图：

**一级分类**：

![](/assets/postAssets/2017/14979381737768.webp)


**二级分类**：

![](/assets/postAssets/2017/14979381802458.webp)


**4\. 可运营**

在正式发布之前，我们进行了灰度，以评估卡顿对用户的影响。收集到的结果是用户平均每天会产生30个 dump 文件，压缩上传大约要 300k 流量。预计正式发布的话会对后台有比较大的压力，对用户也有一定流量损耗。所以必须进行抽样上报。

* 抽样上报：每天抽取不同的用户进行上报，抽样概率是5%。

* 文件上传：被抽中的用户1天仅上传前20个堆栈文件，并且每次上报会进行多文件压缩上传。

* 白名单：对于需要跟进问题的用户，可以在后台配置白名单，强制上报。

另外，为了减少对用户存储空间的影响，卡顿文件仅保存最近7天的记录，过期删除。



## 效果



主线程卡顿监控在微信5.3.1灰度以来，已经成功解决了不少常规手段无法定位的难题，包括：

1. 订阅号更新导致微信切换前台很卡（500+订阅号）

2. 通讯录延迟加载导致偶尔卡一下（1k+好友）



## 他山之石与后续工作





移动客户端性能优化是个很大的话题，也是一个快速发展的领域。我们在这里抛砖引玉，欢迎各位同学一起讨论，相互学习进步。

1. 主线程卡顿跟 iOS 的 0x8badf00d 异常 (failed to resume in time)，或 Android 的 ANR（Application Not Response）类似。这些系统基本的行为的缺点是场景很少，基本上是超时10秒以上才会捕捉到，导致的后果是数据量很少，并且很多卡顿问题是没有覆盖到的。

2. 跟其他同事有了解到另外一种方法，就是 hook了msgSend 把每个函数的耗时记录下来。这个方法思路也很新颖，不过个人觉得性能损耗可能比较大，很难在正式版带上。

3. 如果主线程占用了100%的 CPU，那么怎么保证检测子线程可以有机会跑起来呢？目前我们只对 4S 以上机器（有双 CPU）启用这个功能，还是无法保证检测子线程会被调用。

